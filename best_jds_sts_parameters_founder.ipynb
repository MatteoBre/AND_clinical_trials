{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brescia\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article 27953647  not found.\n",
      "article 27948541  not found.\n",
      "article 27943881  not found.\n",
      "article 27949797  not found.\n",
      "article 27955116  not found.\n",
      "article 27950623  not found.\n",
      "article 27945102  not found.\n",
      "clinical trial NCT02659670  not found.\n",
      "article 27198327  not found.\n"
     ]
    }
   ],
   "source": [
    "import articles.article_fetch as article_fetch\n",
    "import articles.articles_info as article_info\n",
    "from articles import article\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "\n",
    "import pandas as pd\n",
    "from classifiers.random_forest import RandomForest\n",
    "from classifiers.svm_classifier import SVMClassifier\n",
    "from classifiers.mlp_classifier import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import clinical_trials.clinical_trial_fetch as ct_fetch\n",
    "import clinical_trials.clinical_trials_info as ct_info\n",
    "from clinical_trials import clinical_trial\n",
    "\n",
    "import csv\n",
    "import csv_data.csv as csv_data\n",
    "\n",
    "from dataframe import create_dataframe\n",
    "from dataframe import calculate_attributes\n",
    "\n",
    "from common_functions import common_functions\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import importlib\n",
    "\n",
    "# I get the gold standard\n",
    "gold_standard = pd.read_csv('ClinicalPmidsALL.csv', encoding = 'ISO-8859-1', sep = ';')\n",
    "\n",
    "# Now I change the common answer from string to numerical\n",
    "csv_data.numerical_answers(gold_standard)\n",
    "\n",
    "# Let's correct data on csv (common answer sometimes is not correct)\n",
    "csv_data.correctData(gold_standard)\n",
    "\n",
    "# I get all the clinical trials ID and I get the articles in xml\n",
    "Clinical_trials = ct_fetch.get_xml_doms(gold_standard['CT'].tolist())\n",
    "\n",
    "# I get all the PMID and I get the articles in xml\n",
    "PubMed_id_string = list(map(str, gold_standard['PMID'].tolist())) # I get the PMID as list of strings\n",
    "PubMed_articles = article_fetch.fetch_many_articles(PubMed_id_string, local=True) # It takes time to fetch the articles\n",
    "\n",
    "# Let's map every clinical trial to every article (and remove the one that are not present) and common answer\n",
    "df = create_dataframe.get_base_dataframe(gold_standard, Clinical_trials, PubMed_articles)\n",
    "\n",
    "# standard, spacy or stanford\n",
    "ct_org_sample = 'standard'\n",
    "ar_org_sample = 'spacy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lam = gold_standard['CT']\n",
    "#lam.index[lam == 'NCT00003204'].tolist()[0]\n",
    "\n",
    "ct_last_names, ct_first_name_initials, ct_first_names = ct_info.get_all_name_parts(df['CT'].tolist(), gold_standard)\n",
    "\n",
    "df['ct_last_name'], df['ct_first_name_initial'], df['ct_first_name'] = [ct_last_names, ct_first_name_initials, ct_first_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum(x is not None for x in ct_last_names))\n",
    "df = df.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ar_last_names, ar_first_name_initials, ar_first_names = article_info.get_all_name_parts(df['PubMed'].tolist(), gold_standard)\n",
    "\n",
    "df['ar_last_name'], df['ar_first_name_initial'], df['ar_first_name'] = [ar_last_names, ar_first_name_initials, ar_first_names]\n",
    "df.dropna().reset_index(drop = True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCT00730210 doesn't have an organization.\n",
      "NCT02220283 doesn't have an organization.\n"
     ]
    }
   ],
   "source": [
    "# now that we have the base information, let's add other attributes\n",
    "\n",
    "# we get the organization of the principal investigator, so we don't need to pass his/her name\n",
    "ct_organization_names = ct_info.get_all_organization_names(df['CT'].tolist(), df['ct_last_name'].tolist(),\n",
    "                                                           df['ct_first_name_initial'].tolist(),\n",
    "                                                           sample=ct_org_sample)\n",
    "\n",
    "# Now we insert it in the dataframe\n",
    "if('ct_organization' not in df):\n",
    "    df.insert(6, 'ct_organization', ct_organization_names)\n",
    "    \n",
    "\n",
    "# I get the e-mails\n",
    "ct_mails = ct_info.get_all_mails(df['CT'].tolist(), df['ct_last_name'].tolist(), df['ct_first_name_initial'].tolist())\n",
    "\n",
    "if('ct_mail' not in df):\n",
    "    df.insert(7, 'ct_mail', ct_mails)\n",
    "    \n",
    "# I get the year\n",
    "ct_years = ct_info.get_all_years(df['CT'].tolist())\n",
    "    \n",
    "if('ct_year' not in df):\n",
    "    df.insert(8, 'ct_year', ct_years)\n",
    "    \n",
    "# I get the initials of the name\n",
    "ct_initials = ct_info.get_all_initials(df['ct_first_name'].tolist())\n",
    "\n",
    "if('ct_initials' not in df):\n",
    "    df.insert(9, 'ct_initials', ct_initials)\n",
    "    \n",
    "# I get the title\n",
    "ct_titles = ct_info.get_all_titles(df['CT'].tolist())\n",
    "\n",
    "if('ct_title' not in df):\n",
    "    df.insert(10, 'ct_title', ct_titles)\n",
    "    \n",
    "# I get the country and the city\n",
    "ct_countries, ct_cities = ct_info.get_all_countries_and_cities(df['CT'].tolist())\n",
    "\n",
    "if('ct_country' not in df):\n",
    "    df.insert(11, 'ct_country', ct_countries)\n",
    "    \n",
    "if('ct_city' not in df):\n",
    "    df.insert(12, 'ct_city', ct_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n"
     ]
    }
   ],
   "source": [
    "print(sum(x is not None for x in df['ct_mail']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475\n",
      "126\n"
     ]
    }
   ],
   "source": [
    "# Let's add the same attributes for the articles\n",
    "\n",
    "ar_organization_names, ar_locations = article_info.get_all_organizations_locations(df['PubMed'].tolist(),\n",
    "                                                           df['ar_last_name'].tolist(),\n",
    "                                                           df['ar_first_name_initial'].tolist(),\n",
    "                                                           sample = ar_org_sample)\n",
    "df['ar_organization'] = ar_organization_names\n",
    "\n",
    "ar_mails = article_info.get_all_mails(df['PubMed'].tolist(), df['ar_last_name'].tolist(),\n",
    "                                      df['ar_first_name_initial'].tolist())\n",
    "df['ar_mail'] = ar_mails\n",
    "\n",
    "ar_years = article_info.get_all_years(df['PubMed'].tolist())\n",
    "\n",
    "df['ar_year'] = ar_years\n",
    "\n",
    "ar_initials = article_info.get_all_initials(df['ar_first_name'])\n",
    "\n",
    "df['ar_initials'] = ar_initials\n",
    "\n",
    "ar_titles = article_info.get_all_titles(df['PubMed'].tolist())\n",
    "\n",
    "df['ar_title'] = ar_titles\n",
    "df['ar_location'] = ar_locations\n",
    "\n",
    "print(sum(x is not None for x in ar_organization_names))\n",
    "print(sum(x is not None for x in ar_mails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I now calculate useful attributes for the classifiers\n",
    "first_name_equalities = calculate_attributes.get_string_arrays_similarity(df['ct_first_name'].tolist(),\n",
    "                                                                          df['ar_first_name'].tolist())\n",
    "organization_similarities, organization_type_equalities = calculate_attributes.get_organization_similarity_and_type_equality(df['ct_organization'].tolist(),\n",
    "                                                                             df['ar_organization'].tolist(),\n",
    "                                                                             ct_org_sample, ar_org_sample)\n",
    "email_equalities = calculate_attributes.get_arrays_equality(df['ct_mail'].tolist(), df['ar_mail'].tolist())\n",
    "year_differences = calculate_attributes.get_year_differences(df['ct_year'].tolist(), df['ar_year'].tolist())\n",
    "last_name_lengths = calculate_attributes.get_last_name_lengths(df['ct_last_name'].tolist())\n",
    "initials_equality = calculate_attributes.get_arrays_equality(df['ct_initials'].tolist(), df['ar_initials'].tolist())\n",
    "namespace_sizes = calculate_attributes.get_namespace_ambiguities(df['ct_last_name'].tolist(),\n",
    "                                                                 df['ct_first_name_initial'].tolist())\n",
    "country_equalities, city_equalities = calculate_attributes.get_location_equalities(df['ct_country'].tolist(),\n",
    "                                                                                   df['ct_city'].tolist(),\n",
    "                                                                                   df['ar_location'].tolist())\n",
    "\n",
    "# Let's add the attributes to the data frame\n",
    "df['first_name_equality'], df['organization_similarity'] = [first_name_equalities, organization_similarities]\n",
    "df['email_equality'], df['year_difference'] = [email_equalities, year_differences]\n",
    "df['last_name_length'], df['initials_equality'] = [last_name_lengths, initials_equality]\n",
    "df['namespace_size'], df ['country_equality'] = [namespace_sizes, country_equalities]\n",
    "df['city_equality'], df['organization_type_equality'] = [city_equalities, organization_type_equalities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from java_libraries import java_server\n",
    "\n",
    "clinical_trials = df['CT'].tolist()\n",
    "articles = df['PubMed'].tolist()\n",
    "\n",
    "server = java_server.JavaServer()\n",
    "\n",
    "jds = []\n",
    "sts = []\n",
    "for i in range(len(articles)):\n",
    "    ct_texts = clinical_trials[i].get_text()\n",
    "    ct_texts.append(clinical_trials[i].get_title())\n",
    "    ct_texts = \" \".join(ct_texts)\n",
    "\n",
    "    ar_texts = articles[i].get_text()\n",
    "    ar_texts.append(articles[i].get_title())\n",
    "    ar_texts = \" \".join(ar_texts)\n",
    "        \n",
    "    jd = [server.get_jds(ct_texts), server.get_jds(ar_texts)]\n",
    "    jds.append(jd)\n",
    "    st = [server.get_sts(ct_texts), server.get_sts(ar_texts)]\n",
    "    sts.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.close_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_precision = 0\n",
    "\n",
    "from dataframe import calculate_attributes\n",
    "\n",
    "for i in range(1,20):\n",
    "    for kk in range(1,20):\n",
    "        jds_similarities = []\n",
    "        sts_similarities = []\n",
    "        for j in range(len(jds)):\n",
    "            jds_similarities.append(calculate_attributes.get_jds_sts_basic_similarities(jds[j][0][:i],jds[j][1][:i]))\n",
    "        for j in range(len(sts)):\n",
    "            sts_similarities.append(calculate_attributes.get_jds_sts_basic_similarities(sts[j][0][:kk],sts[j][1][:kk]))\n",
    "\n",
    "        df['jds_similarity'] = jds_similarities\n",
    "        df['sts_similarity'] = sts_similarities\n",
    "\n",
    "        # normalize\n",
    "        df['namespace_size'] = common_functions.normalize(df['namespace_size'])\n",
    "        df['last_name_length'] = common_functions.normalize(df['last_name_length'])\n",
    "        df['year_difference'] = common_functions.normalize(df['year_difference'])\n",
    "\n",
    "        # save csv\n",
    "        df_to_save = df.drop(['CT', 'PubMed', 'ct_last_name', 'ct_first_name_initial', 'ct_first_name',\n",
    "              'ct_organization', 'ct_mail', 'ct_year', 'ct_initials', 'ct_title', 'ct_country', 'ct_city', \n",
    "                'ar_last_name','ar_first_name_initial','ar_first_name','ar_organization', 'ar_mail', 'ar_initials',\n",
    "                      'ar_year', 'ar_title', 'ar_location'], axis = 1)\n",
    "        csv = df_to_save.to_csv(index=False)\n",
    "        file = codecs.open(\"dataframe.csv\", \"w\", \"utf-8\")\n",
    "        file.write(csv)\n",
    "        file.close()\n",
    "\n",
    "        # model\n",
    "        df_model = pd.read_csv('dataframe.csv', encoding = 'utf-8')\n",
    "        x = df_model.drop('common_answer', axis=1)\n",
    "        y = df_model['common_answer']\n",
    "\n",
    "        avg_precision = 0\n",
    "        for k in range(100):\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = k)\n",
    "            classifier = RandomForest(0)\n",
    "            classifier.create_model(x_train, y_train)\n",
    "            pred = classifier.predict(x_test)\n",
    "            avg_precision += precision_recall_fscore_support(y_test, pred, average='weighted')[0]\n",
    "        avg_precision /= 100\n",
    "\n",
    "        print(\"i:\",i,\"average precision:\",avg_precision)\n",
    "\n",
    "        if avg_precision > max_precision:\n",
    "            max_precision = avg_precision\n",
    "            best_i = i\n",
    "            best_kk = kk\n",
    "        \n",
    "print(best_i, best_kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_precision = 0\n",
    "\n",
    "for i in range(1,100):\n",
    "    jds_similarities = []\n",
    "    sts_similarities = []\n",
    "    for j in range(len(jds)):\n",
    "        jds_similarities.append(calculate_attributes.get_jds_sts_percentage_similarities(jds[j][0][:i],jds[j][1][:i]))\n",
    "    for j in range(len(sts)):\n",
    "        sts_similarities.append(calculate_attributes.get_jds_sts_percentage_similarities(sts[j][0][:i],sts[j][1][:i]))\n",
    "    \n",
    "    df['jds_similarity'] = jds_similarities\n",
    "    df['sts_similarity'] = sts_similarities\n",
    "\n",
    "    # normalize\n",
    "    df['namespace_size'] = common_functions.normalize(df['namespace_size'])\n",
    "    df['last_name_length'] = common_functions.normalize(df['last_name_length'])\n",
    "    df['year_difference'] = common_functions.normalize(df['year_difference'])\n",
    "\n",
    "    # save csv\n",
    "    df_to_save = df.drop(['CT', 'PubMed', 'ct_last_name', 'ct_first_name_initial', 'ct_first_name',\n",
    "          'ct_organization', 'ct_mail', 'ct_year', 'ct_initials', 'ct_title', 'ct_country', 'ct_city', \n",
    "            'ar_last_name','ar_first_name_initial','ar_first_name','ar_organization', 'ar_mail', 'ar_initials',\n",
    "                  'ar_year', 'ar_title', 'ar_location'], axis = 1)\n",
    "    csv = df_to_save.to_csv(index=False)\n",
    "    file = codecs.open(\"dataframe.csv\", \"w\", \"utf-8\")\n",
    "    file.write(csv)\n",
    "    file.close()\n",
    "\n",
    "    # model\n",
    "    df_model = pd.read_csv('dataframe.csv', encoding = 'utf-8')\n",
    "    x = df_model.drop('common_answer', axis=1)\n",
    "    y = df_model['common_answer']\n",
    "\n",
    "    avg_precision = 0\n",
    "    for k in range(100):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = k)\n",
    "        classifier = RandomForest(0)\n",
    "        classifier.create_model(x_train, y_train)\n",
    "        pred = classifier.predict(x_test)\n",
    "        avg_precision += precision_recall_fscore_support(y_test, pred, average='weighted')[0]\n",
    "    avg_precision /= 100\n",
    "\n",
    "    print(\"i:\",i,\"average precision:\",avg_precision)\n",
    "\n",
    "    if avg_precision > max_precision:\n",
    "        max_precision = avg_precision\n",
    "        best_i = i\n",
    "        \n",
    "print(best_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_precision = 0\n",
    "\n",
    "for i in range(1,100):\n",
    "    jds_similarities = []\n",
    "    sts_similarities = []\n",
    "    for j in range(len(jds)):\n",
    "        jds_similarities.append(calculate_attributes.get_jds_sts_ranking_similarities(jds[j][0][:i],jds[j][1][:i]))\n",
    "    for j in range(len(sts)):\n",
    "        sts_similarities.append(calculate_attributes.get_jds_sts_ranking_similarities(sts[j][0][:i],sts[j][1][:i]))\n",
    "    \n",
    "    df['jds_similarity'] = jds_similarities\n",
    "    df['sts_similarity'] = sts_similarities\n",
    "\n",
    "    # normalize\n",
    "    df['namespace_size'] = common_functions.normalize(df['namespace_size'])\n",
    "    df['last_name_length'] = common_functions.normalize(df['last_name_length'])\n",
    "    df['year_difference'] = common_functions.normalize(df['year_difference'])\n",
    "\n",
    "    # save csv\n",
    "    df_to_save = df.drop(['CT', 'PubMed', 'ct_last_name', 'ct_first_name_initial', 'ct_first_name',\n",
    "          'ct_organization', 'ct_mail', 'ct_year', 'ct_initials', 'ct_title', 'ct_country', 'ct_city', \n",
    "            'ar_last_name','ar_first_name_initial','ar_first_name','ar_organization', 'ar_mail', 'ar_initials',\n",
    "                  'ar_year', 'ar_title', 'ar_location'], axis = 1)\n",
    "    csv = df_to_save.to_csv(index=False)\n",
    "    file = codecs.open(\"dataframe.csv\", \"w\", \"utf-8\")\n",
    "    file.write(csv)\n",
    "    file.close()\n",
    "\n",
    "    # model\n",
    "    df_model = pd.read_csv('dataframe.csv', encoding = 'utf-8')\n",
    "    x = df_model.drop('common_answer', axis=1)\n",
    "    y = df_model['common_answer']\n",
    "\n",
    "    avg_precision = 0\n",
    "    for k in range(100):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = k)\n",
    "        classifier = RandomForest(0)\n",
    "        classifier.create_model(x_train, y_train)\n",
    "        pred = classifier.predict(x_test)\n",
    "        avg_precision += precision_recall_fscore_support(y_test, pred, average='weighted')[0]\n",
    "    avg_precision /= 100\n",
    "\n",
    "    print(\"i:\",i,\"average precision:\",avg_precision)\n",
    "\n",
    "    if avg_precision > max_precision:\n",
    "        max_precision = avg_precision\n",
    "        best_i = i\n",
    "        \n",
    "print(best_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 4 average precision: kk: 1 0.8871453039693695\n",
      "i: 4 average precision: kk: 2 0.8849539626403151\n",
      "i: 4 average precision: kk: 3 0.8828043849315894\n",
      "i: 4 average precision: kk: 4 0.8834250961484948\n",
      "i: 4 average precision: kk: 5 0.8857022645417897\n",
      "i: 4 average precision: kk: 6 0.8838896215698352\n",
      "i: 4 average precision: kk: 7 0.8838774557081963\n",
      "i: 4 average precision: kk: 8 0.8839015357802716\n",
      "i: 4 average precision: kk: 9 0.8858048558172129\n",
      "i: 4 average precision: kk: 10 0.8904061853664359\n",
      "i: 4 average precision: kk: 11 0.8892342634239935\n",
      "i: 4 average precision: kk: 12 0.8887784118744395\n",
      "i: 4 average precision: kk: 13 0.8869539583629461\n",
      "i: 4 average precision: kk: 14 0.88708454395335\n",
      "i: 4 average precision: kk: 15 0.8874596249745109\n",
      "i: 4 average precision: kk: 16 0.8867620505722483\n",
      "i: 4 average precision: kk: 17 0.8879746673054871\n",
      "i: 4 average precision: kk: 18 0.8871773611057993\n",
      "i: 4 average precision: kk: 19 0.8870156283506719\n",
      "i: 4 average precision: kk: 20 0.8876676275638232\n",
      "i: 4 average precision: kk: 21 0.8862214568138972\n",
      "i: 4 average precision: kk: 22 0.8880859248284836\n",
      "i: 4 average precision: kk: 23 0.8894704616900809\n",
      "i: 4 average precision: kk: 24 0.8912209445117693\n",
      "i: 4 average precision: kk: 25 0.8899509423982618\n",
      "i: 4 average precision: kk: 26 0.8931373467611116\n",
      "i: 4 average precision: kk: 27 0.8907789054129456\n",
      "i: 4 average precision: kk: 28 0.8907018556509455\n",
      "i: 4 average precision: kk: 29 0.8909200268037486\n",
      "i: 4 average precision: kk: 30 0.8927780480479645\n",
      "i: 4 average precision: kk: 31 0.8910149108475619\n",
      "i: 4 average precision: kk: 32 0.8906449143746081\n",
      "i: 4 average precision: kk: 33 0.8890209361081419\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8a5598e14e14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mavg_precision\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Diploma\\classifiers\\random_forest.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(self, x_train, y_train)\u001b[0m\n\u001b[0;32m     14\u001b[0m         self.rfc = RandomForestClassifier(random_state=self.random_state, n_estimators=500, max_depth=None,\n\u001b[0;32m     15\u001b[0m                                           min_samples_split=2, max_features='auto')\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanSave\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 328\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                 classes_k, y_encoded[:, k] = np.unique(y[:, k],\n\u001b[1;32m--> 152\u001b[1;33m                                                        return_inverse=True)\n\u001b[0m\u001b[0;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses_k\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[0mFind\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munique\u001b[0m \u001b[0melements\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignoring\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \"\"\"\n\u001b[1;32m--> 273\u001b[1;33m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[0moptional_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_precision = 0\n",
    "\n",
    "from dataframe import calculate_attributes\n",
    "\n",
    "for i in range(4,50):\n",
    "    for kk in range(1,50):\n",
    "        jds_similarities = []\n",
    "        sts_similarities = []\n",
    "        for j in range(len(jds)):\n",
    "            jds_similarities.append(calculate_attributes.get_jds_sts_percentage_ranking_similarities(jds[j][0][:i],jds[j][1][:i]))\n",
    "        for j in range(len(sts)):\n",
    "            sts_similarities.append(calculate_attributes.get_jds_sts_percentage_ranking_similarities(sts[j][0][:kk],sts[j][1][:kk]))\n",
    "\n",
    "        df['jds_similarity'] = jds_similarities\n",
    "        df['sts_similarity'] = sts_similarities\n",
    "\n",
    "        # normalize\n",
    "        df['namespace_size'] = common_functions.normalize(df['namespace_size'])\n",
    "        df['last_name_length'] = common_functions.normalize(df['last_name_length'])\n",
    "        df['year_difference'] = common_functions.normalize(df['year_difference'])\n",
    "\n",
    "        # save csv\n",
    "        df_to_save = df.drop(['CT', 'PubMed', 'ct_last_name', 'ct_first_name_initial', 'ct_first_name',\n",
    "              'ct_organization', 'ct_mail', 'ct_year', 'ct_initials', 'ct_title', 'ct_country', 'ct_city', \n",
    "                'ar_last_name','ar_first_name_initial','ar_first_name','ar_organization', 'ar_mail', 'ar_initials',\n",
    "                      'ar_year', 'ar_title', 'ar_location'], axis = 1)\n",
    "        csv = df_to_save.to_csv(index=False)\n",
    "        file = codecs.open(\"dataframe.csv\", \"w\", \"utf-8\")\n",
    "        file.write(csv)\n",
    "        file.close()\n",
    "\n",
    "        # model\n",
    "        df_model = pd.read_csv('dataframe.csv', encoding = 'utf-8')\n",
    "        x = df_model.drop('common_answer', axis=1)\n",
    "        y = df_model['common_answer']\n",
    "\n",
    "        avg_precision = 0\n",
    "        for k in range(15):\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = k)\n",
    "            classifier = RandomForest(0)\n",
    "            classifier.create_model(x_train, y_train)\n",
    "            pred = classifier.predict(x_test)\n",
    "            avg_precision += precision_recall_fscore_support(y_test, pred, average='weighted')[0]\n",
    "        avg_precision /= 15\n",
    "\n",
    "        print(\"i:\",i,\"average precision:\",\"kk:\",kk,avg_precision)\n",
    "\n",
    "        if avg_precision > max_precision:\n",
    "            max_precision = avg_precision\n",
    "            best_i = i\n",
    "            best_kk = kk\n",
    "        \n",
    "print(best_i, best_kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(article_info)\n",
    "importlib.reload(ct_info)\n",
    "importlib.reload(calculate_attributes)\n",
    "importlib.reload(clinical_trial)\n",
    "importlib.reload(common_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
